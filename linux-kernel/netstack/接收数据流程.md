## 链路层接收数据

### 网卡(PCI 设备的注册)
网卡的net_device就是在这个函数里面初始化的并注册到内核的。

### 网卡链路状态检测
当网卡链路状态变化时（如断开或连上），网卡会通知驱动程序或者由驱动程序去查询网卡的相关寄存器位（例如在timeout时区查询这些位），然后再netif_carrier_on/off去通知内核这个变化。
```c

|
|---------->

```

```c
net/sched/sch_generic.c
/**
 *	netif_carrier_on - set carrier
 *	@dev: network device
 *
 * Device has detected acquisition of carrier.
 */
void netif_carrier_on(struct net_device *dev)
{
	if (test_and_clear_bit(__LINK_STATE_NOCARRIER, &dev->state)) {
		if (dev->reg_state == NETREG_UNINITIALIZED)
			return;
		atomic_inc(&dev->carrier_up_count);
		linkwatch_fire_event(dev);
		if (netif_running(dev))
			__netdev_watchdog_up(dev);
	}
}
EXPORT_SYMBOL(netif_carrier_on);

```

```c
net/core/link_watch.c
void linkwatch_fire_event(struct net_device *dev)
{
	bool urgent = linkwatch_urgent_event(dev);

	if (!test_and_set_bit(__LINK_STATE_LINKWATCH_PENDING, &dev->state)) {
		linkwatch_add_event(dev);
	} else if (!urgent)
		return;

	linkwatch_schedule_work(urgent);
}
EXPORT_SYMBOL(linkwatch_fire_event);

```

```c
static void linkwatch_schedule_work(int urgent)
{
	unsigned long delay = linkwatch_nextevent - jiffies;

	if (test_bit(LW_URGENT, &linkwatch_flags))
		return;

	/* Minimise down-time: drop delay for up event. */
	if (urgent) {
		if (test_and_set_bit(LW_URGENT, &linkwatch_flags))
			return;
		delay = 0;
	}

	/* If we wrap around we'll delay it by at most HZ. */
	if (delay > HZ)
		delay = 0;

	/*
	 * If urgent, schedule immediate execution; otherwise, don't
	 * override the existing timer.
	 */
	if (test_bit(LW_URGENT, &linkwatch_flags))
		mod_delayed_work(system_wq, &linkwatch_work, 0);
	else
		schedule_delayed_work(&linkwatch_work, delay);
}

static void linkwatch_event(struct work_struct *dummy);
static DECLARE_DELAYED_WORK(linkwatch_work, linkwatch_event);

```

然后调用linkwatch_schedule_work(urgent);-->调用schedule_delayed_work(&linkwatch_work, delay);由内核线程去处理这些事件。最终由linkwatch_run_queue(void)去完成这些处理工作。

```c
/* Must be called with the rtnl semaphore held */
void linkwatch_run_queue(void)
{
	__linkwatch_run_queue(0);
}


static void linkwatch_event(struct work_struct *dummy)
{
	rtnl_lock();
	__linkwatch_run_queue(time_after(linkwatch_nextevent, jiffies));
	rtnl_unlock();
}

```


```c
static void __linkwatch_run_queue(int urgent_only)
{
	struct net_device *dev;
	LIST_HEAD(wrk);

	/*
	 * Limit the number of linkwatch events to one
	 * per second so that a runaway driver does not
	 * cause a storm of messages on the netlink
	 * socket.  This limit does not apply to up events
	 * while the device qdisc is down.
	 */
	if (!urgent_only)
		linkwatch_nextevent = jiffies + HZ;
	/* Limit wrap-around effect on delay. */
	else if (time_after(linkwatch_nextevent, jiffies + HZ))
		linkwatch_nextevent = jiffies;

	clear_bit(LW_URGENT, &linkwatch_flags);

	spin_lock_irq(&lweventlist_lock);
	list_splice_init(&lweventlist, &wrk);

	while (!list_empty(&wrk)) {

		dev = list_first_entry(&wrk, struct net_device, link_watch_list);
		list_del_init(&dev->link_watch_list);

		if (urgent_only && !linkwatch_urgent_event(dev)) {
			list_add_tail(&dev->link_watch_list, &lweventlist);
			continue;
		}
		spin_unlock_irq(&lweventlist_lock);
		linkwatch_do_dev(dev);
		spin_lock_irq(&lweventlist_lock);
	}

	if (!list_empty(&lweventlist))
		linkwatch_schedule_work(0);
	spin_unlock_irq(&lweventlist_lock);
}

```

```c
static void linkwatch_do_dev(struct net_device *dev)
{
	/*
	 * Make sure the above read is complete since it can be
	 * rewritten as soon as we clear the bit below.
	 */
	smp_mb__before_atomic();

	/* We are about to handle this device,
	 * so new events can be accepted
	 */
	clear_bit(__LINK_STATE_LINKWATCH_PENDING, &dev->state);

	rfc2863_policy(dev);
	if (dev->flags & IFF_UP && netif_device_present(dev)) {
		if (netif_carrier_ok(dev))
			dev_activate(dev);
		else
			dev_deactivate(dev);

		netdev_state_change(dev);
	}
	dev_put(dev);
}

```

可以看到,它的最主要工作之一就是 netdev_state_change(dev):

```c
/**
 *	netdev_state_change - device changes state
 *	@dev: device to cause notification
 *
 *	Called to indicate a device has changed state. This function calls
 *	the notifier chains for netdev_chain and sends a NEWLINK message
 *	to the routing socket.
 */
void netdev_state_change(struct net_device *dev)
{
	if (dev->flags & IFF_UP) {
		struct netdev_notifier_change_info change_info = {
			.info.dev = dev,
		};

		call_netdevice_notifiers_info(NETDEV_CHANGE,
					      &change_info.info);
		rtmsg_ifinfo(RTM_NEWLINK, dev, 0, GFP_KERNEL);
	}
}
EXPORT_SYMBOL(netdev_state_change);

```
这个函数通知注册到 netdev_chain 链表的所有子系统,这个网卡的链路状态有了变化。就是说,如果某个子系统对网卡的链路状态变化感兴趣,它就可以注册到进这个链表,在变化产生时,内核便会通知这些子系统。
它会调用"rtmsg_ifinfo"函数，向路由守护进程发送一个新的链路信息。这个函数可能会更新路由表，以反映设备状态的变化。

注意:
a. 它只会在网卡状态为 UP 时,才会发出通知,因为,如果状态为 DOWN,网卡链路的状态改变也没什么意义。
b. 每个见网卡的这些状态变化的事件 lw_event 是不会队列的,即每个网卡只有一个事件的实例在队列中。还有由上面看到的 lw_event 结构,它只是包含发生状态变化的网卡设备,而没有包含它是链上或是断开的状状参数。

### 数据包的接收
这个数据结构同时用于接收与发送数据包。

```c
struct softnet_data {
    struct list_head    poll_list;
    struct sk_buff_head process_queue;

    /* stats */
    unsigned int        processed;
    unsigned int        time_squeeze;
    unsigned int        received_rps;
#ifdef CONFIG_RPS
    struct softnet_data *rps_ipi_list;
#endif
#ifdef CONFIG_NET_FLOW_LIMIT
    struct sd_flow_limit __rcu *flow_limit;
#endif
    struct Qdisc        *output_queue;
    struct Qdisc        **output_queue_tailp;
    struct sk_buff      *completion_queue;
#ifdef CONFIG_XFRM_OFFLOAD
    struct sk_buff_head xfrm_backlog;
#endif
#ifdef CONFIG_RPS
    /* input_queue_head should be written by cpu owning this struct,
     * and only read by other cpus. Worth using a cache line.
     */
    unsigned int        input_queue_head ____cacheline_aligned_in_smp;

    /* Elements below can be accessed between CPUs for RPS/RFS */
    call_single_data_t  csd ____cacheline_aligned_in_smp;
    struct softnet_data *rps_ipi_next;
    unsigned int        cpu;
    unsigned int        input_queue_tail;
#endif
    unsigned int        dropped;
    struct sk_buff_head input_pkt_queue;
    struct napi_struct  backlog;

};

```

```c
// input_pkt_queue是一个sk_buff队列，用于存放输入的网络数据包。
struct sk_buff_head input_pkt_queue;
// poll_list是一个链表，用于存放等待被处理的网络数据包。
struct list_head poll_list;
//
struct net_device backlog_dev;
```
#### NON-NAPI方式

#### NAPI方式

#### net_rx_action()

中断下半部执行：

```c
net/core/dev.c
static int __init net_dev_init(void)
{
	open_softirq(NET_TX_SOFTIRQ, net_tx_action);
	open_softirq(NET_RX_SOFTIRQ, net_rx_action);

}

```
`static __latent_entropy void net_rx_action(struct softirq_action *h){}`
这段代码是Linux内核中用于处理网络接收的函数。它主要做以下几件事情：
* 获取当前CPU上的软网络数据（softnet_data），并禁用本地中断。
* 将当前CPU上的软中断动作列表（softirq_action）移动到一个新的列表中。
* 对列表中的每一项（napi_struct）进行轮询，也就是调用napi_poll函数。如果没有足够的时间或者budget来完成所有的工作，就跳出循环。
* 重新启用本地中断，然后将剩余的任务添加回原来的列表。
* 最后，检查是否还有待处理的任务，如果有，就再次引发NET_RX_SOFTIRQ软中断。
* 释放可能被napi_poll函数分配的任何未使用的skb

#### poll函数
每个驱动都自己实现：

然后调用int netif_receive_skb(struct sk_buff *skb)


####  netif_receive_skb
net/core/dev.c文件中。这是一个辅助函数,用于在 poll 中处理接收到的帧。它主要是向各个已注册的协议处理例程发送一个 SKB。
每个协议的类型由一个 packet_type 结构表示:
```c
/**
 *	netif_receive_skb - process receive buffer from network
 *	@skb: buffer to process
 *
 *	netif_receive_skb() is the main receive data processing function.
 *	It always succeeds. The buffer may be dropped during processing
 *	for congestion control or by the protocol layers.
 *
 *	This function may only be called from softirq context and interrupts
 *	should be enabled.
 *
 *	Return values (usually ignored):
 *	NET_RX_SUCCESS: no congestion
 *	NET_RX_DROP: packet was dropped
 */
int netif_receive_skb(struct sk_buff *skb)
{
	int ret;

	trace_netif_receive_skb_entry(skb);

	ret = netif_receive_skb_internal(skb);
	trace_netif_receive_skb_exit(ret);

	return ret;
}
EXPORT_SYMBOL(netif_receive_skb);

static int netif_receive_skb_internal(struct sk_buff *skb)
{
	int ret;

	net_timestamp_check(netdev_tstamp_prequeue, skb);

	if (skb_defer_rx_timestamp(skb))
		return NET_RX_SUCCESS;

	if (static_branch_unlikely(&generic_xdp_needed_key)) {
		int ret;

		preempt_disable();
		rcu_read_lock();
		ret = do_xdp_generic(rcu_dereference(skb->dev->xdp_prog), skb);
		rcu_read_unlock();
		preempt_enable();

		if (ret != XDP_PASS)
			return NET_RX_DROP;
	}

	rcu_read_lock();
#ifdef CONFIG_RPS
	if (static_key_false(&rps_needed)) {
		struct rps_dev_flow voidflow, *rflow = &voidflow;
		int cpu = get_rps_cpu(skb->dev, skb, &rflow);

		if (cpu >= 0) {
			ret = enqueue_to_backlog(skb, cpu, &rflow->last_qtail);
			rcu_read_unlock();
			return ret;
		}
	}
#endif
	ret = __netif_receive_skb(skb);
	rcu_read_unlock();
	return ret;
}


static int __netif_receive_skb(struct sk_buff *skb)
{
	int ret;

	if (sk_memalloc_socks() && skb_pfmemalloc(skb)) {
		unsigned int noreclaim_flag;

		/*
		 * PFMEMALLOC skbs are special, they should
		 * - be delivered to SOCK_MEMALLOC sockets only
		 * - stay away from userspace
		 * - have bounded memory usage
		 *
		 * Use PF_MEMALLOC as this saves us from propagating the allocation
		 * context down to all allocation sites.
		 */
		noreclaim_flag = memalloc_noreclaim_save();
		ret = __netif_receive_skb_one_core(skb, true);
		memalloc_noreclaim_restore(noreclaim_flag);
	} else
		ret = __netif_receive_skb_one_core(skb, false);

	return ret;
}

static int __netif_receive_skb_one_core(struct sk_buff *skb, bool pfmemalloc)
{
	struct net_device *orig_dev = skb->dev;
	struct packet_type *pt_prev = NULL;
	int ret;

	ret = __netif_receive_skb_core(skb, pfmemalloc, &pt_prev);
	if (pt_prev)
		ret = pt_prev->func(skb, skb->dev, pt_prev, orig_dev);
	return ret;
}

static int __netif_receive_skb_core(struct sk_buff *skb, bool pfmemalloc,
				    struct packet_type **ppt_prev)
{


	list_for_each_entry_rcu(ptype, &skb->dev->ptype_all, list) {
		if (pt_prev)
			ret = deliver_skb(skb, pt_prev, orig_dev);
		pt_prev = ptype;
	}

}

static inline int deliver_skb(struct sk_buff *skb,
			      struct packet_type *pt_prev,
			      struct net_device *orig_dev)
{
	if (unlikely(skb_orphan_frags_rx(skb, GFP_ATOMIC)))
		return -ENOMEM;
	refcount_inc(&skb->users);
	return pt_prev->func(skb, skb->dev, pt_prev, orig_dev);
}

```

netif_receive_skb()的主要作用体现在两个遍历链表的操作中,其中之一为遍历 ptype_all 链,这些为注册到内核的一些 sniffer,将上传给这些 sniffer,另一个就是遍历 ptype_base,这个就是具体的协议类型。假高如上图如示,当 eth1 接收到一个 IP 数据包时,它首先分别发送一份副本给两个 ptype_all 链表中的 packet_type,它们都由 package_rcv 处理,然后再根据HASH 值,在遍历另一个 HASH 表时,发送一份给类型为 ETH_P_IP 的类型,它由 ip_rcv处理。如果这个链中还注册有其它 IP 层的协议,它也会同时发送一个副本给它。
deliver_skb只是一个包装函数,它只去执行相应 packet_type 里的 func 处理函数,如对于ETH_P_IP 类型,由上面可以看到,它执行的就是 ip_rcv 了。

所用到的协议在系统或模块加载的时候初始化,如 IP 协议:

```c
static int __init inet_init(void)
{
    dev_add_pack(&ip_packet_type);

}

void dev_add_pack(struct packet_type *pt)
{
	struct list_head *head = ptype_head(pt);

	spin_lock(&ptype_lock);
	list_add_rcu(&pt->list, head);
	spin_unlock(&ptype_lock);
}
EXPORT_SYMBOL(dev_add_pack);

```
这段代码是一个网络驱动程序的部分，用于向一个数据结构添加一个包。这个数据结构是一个双向循环链表，每个节点都是一个packet_type类型的结构体。
首先，定义了一个函数dev_add_pack，该函数接收一个指向packet_type结构体的指针pt作为参数。然后，通过调用ptype_head函数，获取到链表的头指针，并将其赋值给head。
然后，进行了一个关键操作，spin_lock函数用于获取一个内核自旋锁，确保在此期间，不会有其他进程或中断处理程序对数据结构进行修改。这是因为在多处理器系统中，对共享数据结构的并发访问可能会导致数据不一致。
获取到锁之后，就可以安全地对数据结构进行操作了。这里使用的是list_add_rcu函数，该函数将新的packet_type结构体添加到链表中。
最后，调用spin_unlock函数释放锁，表示已经完成对数据结构的操作，其他进程或中断处理程序可以继续访问数据结构了。
这段代码的主要功能是向一个数据结构添加一个包，同时确保了在操作过程中的并发安全性。


### 中断处理接收网卡数据流程


```c

__do_softirq
		--> h->action(h);

在int __init net_dev_init(void)
	--> open_softirq(NET_RX_SOFTIRQ, net_rx_action);

include/linux/interrupt.h
enum
{
	HI_SOFTIRQ=0,
	TIMER_SOFTIRQ,
	NET_TX_SOFTIRQ,
	NET_RX_SOFTIRQ,
	BLOCK_SOFTIRQ,
	IRQ_POLL_SOFTIRQ,
	TASKLET_SOFTIRQ,
	SCHED_SOFTIRQ,
	HRTIMER_SOFTIRQ, /* Unused, but kept as tools rely on the
			    numbering. Sigh! */
	RCU_SOFTIRQ,    /* Preferable RCU should always be the last softirq */

	NR_SOFTIRQS
};

struct softirq_action
{
	void	(*action)(struct softirq_action *);
};

extern void open_softirq(int nr, void (*action)(struct softirq_action *));

/**
这段代码定义了一个名为open_softirq的函数，该函数接收两个参数：一个整数nr和一个指向函数的指针action。函数softirq_vec是一个数组，每个元素都是一个结构体softirq_action。这段代码的作用是将参数action赋值给数组softirq_vec中索引为nr的元素的action成员。

在Linux内核中， softirq 是一种特殊类型的中断，它不直接由硬件设备产生，而是由软件自行生成的中断。open_softirq函数就是用来注册一个softirq处理函数的。当softirq被触发时，就会调用对应的处理函数。

注意：以上解释基于Linux内核中的实现，其他操作系统或者编程语言可能有所不同。
*/
void open_softirq(int nr, void (*action)(struct softirq_action *))
{
	softirq_vec[nr].action = action;
}

就会调用对应的处理函数：net_rx_action

这个函数中剩下的核心逻辑是获取当前CPU变量softnet_data，对其poll_list进行遍历，然后执行到网卡驱动注册的poll函数。

```

调用对应驱动的poll函数，例如：
```c
static __latent_entropy void net_rx_action(struct softirq_action *h)
--> budget -= napi_poll(n, &repoll);
		--> work = n->poll(n, weight);
			--> static int igb_poll(struct napi_struct *napi, int budget) :例如对应的pull函数调用igb_poll:drivers/net/ethernet/intel/igb/igb_main.c
				--> int cleaned = igb_clean_rx_irq(q_vector, budget);
					--> rx_buffer = igb_get_rx_buffer(rx_ring, size);
					--> igb_is_non_eop(rx_ring, rx_desc) :数据帧从RingBuffer取下来。
					--> napi_gro_receive(&q_vector->napi, skb);//net/core/dev.c
```


```c
//net/core/dev.c
gro_result_t napi_gro_receive(struct napi_struct *napi, struct sk_buff *skb)
{
	gro_result_t ret;

	skb_mark_napi_id(skb, napi);
	trace_napi_gro_receive_entry(skb);

	skb_gro_reset_offset(skb);

	ret = napi_skb_finish(dev_gro_receive(napi, skb), skb);
	trace_napi_gro_receive_exit(ret);

	return ret;
}
EXPORT_SYMBOL(napi_gro_receive);

static gro_result_t napi_skb_finish(gro_result_t ret, struct sk_buff *skb)
{
	switch (ret) {
	case GRO_NORMAL:
		if (netif_receive_skb_internal(skb))
			ret = GRO_DROP;
		break;

	case GRO_DROP:
		kfree_skb(skb);
		break;

	case GRO_MERGED_FREE:
		if (NAPI_GRO_CB(skb)->free == NAPI_GRO_FREE_STOLEN_HEAD)
			napi_skb_free_stolen_head(skb);
		else
			__kfree_skb(skb);
		break;

	case GRO_HELD:
	case GRO_MERGED:
	case GRO_CONSUMED:
		break;
	}

	return ret;
}
```

napi_skb_finish调用怎么进入协议栈处理：

```c
// net/core/dev.c
napi_skb_finish(gro_result_t ret, struct sk_buff *skb)
|--> int netif_receive_skb_internal(struct sk_buff *skb)
	|--> ret = __netif_receive_skb(skb);
		|--> int __netif_receive_skb_one_core(struct sk_buff *skb, bool pfmemalloc)
			|--> list_for_each_entry_rcu(ptype, &ptype_all, list)
			|--> ret = deliver_skb(skb, pt_prev, orig_dev);
				--> return pt_prev->func(skb, skb->dev, pt_prev, orig_dev);
			|-->ret = pt_prev->func(skb, skb->dev, pt_prev, orig_dev);
```
`net/ipv4/af_inet.c`文件中将`.func = ip_rcv`,所以在调用`pt_prev->func(skb, skb->dev, pt_prev, orig_dev)`;就可以直接调用`ip_rcv`,或`arp_rcv`等协议处理函数中进行处理。再通过`.handler =	udp_rcv`,传输到`udp_rcv`进一步处理。
```c
static struct packet_type ip_packet_type __read_mostly = {
	.type = cpu_to_be16(ETH_P_IP),
	.func = ip_rcv,
	.list_func = ip_list_rcv,
};

static struct packet_type arp_packet_type __read_mostly = {
	.type =	cpu_to_be16(ETH_P_ARP),
	.func =	arp_rcv,
};

static struct net_protocol udp_protocol = {
	.early_demux =	udp_v4_early_demux,
	.early_demux_handler =	udp_v4_early_demux,
	.handler =	udp_rcv,
	.err_handler =	udp_err,
	.no_policy =	1,
	.netns_ok =	1,
};
```

## 网络协议栈处理（接收数据流程）

在函数中，
netif_receive_skb会根据包的协议进行处理，假如是UDP包，将包依次送到ip_rcv或者arp_rcv等进行协议对应处理函数中。
```c
__netif_receive_skb
	---->  __netif_receive_skb_one_core
			----> arp_rcv
			----> ip_rcv
				----> tcp_v4_rcv()
				----> udp_rcv()
					---> 用户进程skb接收队列，用户进程被唤醒。
```

tcpdump抓包实现：list_for_each_entry_rcu(ptype, &ptype_all, list) （net/core/dev.c）函数中实现，这就是将数据送入抓包点，tcpdump就是从这个入口获取包的。



### IP层处理

首先分析：ip_rcv函数。
这段代码是一个网络接收函数，它接收到的数据包会经过一系列处理后再返回。
首先，定义了一些变量：`skb` 是 socket buffer，用于存放接收到的数据包；`dev` 是设备，可能是物理设备或者虚拟设备；`pt` 是 packet type，表示数据包类型；`orig_dev` 是原始设备，可能与 `dev` 相同，也可能不同。
然后，获取网络设备的网络命名空间 `net`。
接下来，调用 `ip_rcv_core` 函数处理数据包，并将结果赋值给 `skb`。如果处理后 `skb` 为空，则返回 `NET_RX_DROP`，表示丢弃此数据包。
最后，调用 `NF_HOOK` 函数进行网络过滤。这个函数会根据配置的规则对数据包进行过滤和修改，然后调用 `ip_rcv_finish` 函数进行最后的处理。
总的来说，这段代码的主要工作是接收网络数据包，并进行一系列的处理。
```c
net/ipv4/ip_input.c
int ip_rcv(struct sk_buff *skb, struct net_device *dev, struct packet_type *pt,
	   struct net_device *orig_dev)
{
	struct net *net = dev_net(dev);

	skb = ip_rcv_core(skb, net);
	if (skb == NULL)
		return NET_RX_DROP;
	return NF_HOOK(NFPROTO_IPV4, NF_INET_PRE_ROUTING,
		       net, NULL, skb, dev, NULL,
		       ip_rcv_finish);
}
```

在 Linux 内核中，ip_rcv_core 函数是用于处理接收的IP数据包的核心函数。它是 IP 层的一个重要函数，位于网络层实现的一系列函数中。
ip_rcv_core 函数的功能主要包括以下几个方面：
1. 解析和验证接收到的 IP 数据包的头部。它会检查 IP 头部


UDP协议处理

