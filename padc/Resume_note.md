# 1. 微内核架构

## 1.1. futex锁机制实现--用户态锁实现
### 1.1.1. 难点
futex：整个实现调用链设计硬件原子\内核并发、调度器、内核锁实现机制（中断锁，关调度等）等。
只要一个环节与边界条件没对齐，就会出现丢唤醒、优先级反转等。这才是 futex 被称为“用户态一行代码，内核态十年补丁”的原因。

用户态对共享变量的写入，必须让内核在“检查该变量”与“把线程挂起”这两个动作之间看得见，否则就永远睡死。

### 1.1.2. 丢唤醒
弱内存模型下，user 与 kernel 的内存序必须配对：
Linux 用 set_user() 里隐式 DMB，Fuchsia 要在 zx_futex_wait 入口显式 atomic_thread_fence(memory_order_seq_cst)，否则 *uaddr 检查与真正挂起之间可能重入，造成**“丢唤醒”**。

用户态对共享变量的写入，必须让内核再“检查该变量”与线程挂起这两个动作之间看得见，否则就会永远睡死。
五fence的坏序列（若内存模型CPU允许重排）
```cpp
// 用户态解锁路径
unlock() {
    lock->uaddr = 0;            // ① store 0
    // 没有内存屏障
}

// 用户态加锁路径
lock() {
    if (CAS(&lock->uaddr, 0, 1)) return;   // ② fast-path 成功
    // slow path
    for (;;) {
        int v = lock->uaddr;               // ③ load 当前值
        if (v != 0 &&
            syscall(SYS_futex, &lock->uaddr, FUTEX_WAIT, v, NULL) == 0)
            break;
    }
}
```
cpu允许把1的store延迟到之后执行。出现以下时许：
```shell
CPU-0 解锁线程                  CPU-1 加锁线程
─────────────────              ─────────────────
① store 0 进入 ST buffer
                               ③ load uaddr 拿到 1
                               进入内核：
                               ├─ 内核读 *uaddr 得到 1
                               ├─ 校验通过
                               ├─ 把线程设为 TASK_INTERRUPTIBLE
                               ├─ 调度走 —— 此刻**真正睡眠**
② ST buffer 终于刷到 cache
                               → 解锁线程的 0 才被别的 CPU 看见
                               → 没人再 wake，永远睡死
```
这就是典型的（Lost wake-up）内存检查值与挂起之间，用户态的写入被延迟或未完成，就被调度切换，导致检查通过但实际已生效，没人再唤醒，导致线程回收时，触发未被唤醒的线程死等待，系统崩溃。
所以保证内核与用户态的执行内存序对齐，让写入-刷回-检查-挂起按照顺序执行。

根本原因就是：弱内存模型 CPU 允许 store 延迟、load 提前，检查时值是对的，但真正的 0 还在 Store Buffer里，内核一挂起就永远没人再唤醒——睡死线程就是这么来的。

保证拿锁的线程唤醒时，在内核调用过程，整个过程不被调度，也就是睡眠路径根本不会被走到，丢唤醒自然消失。

### 1.1.3. 扩展
CAS机制与CMP原子比较修改：读-改-写一次性原子决策，成功修改为期望新状态（值），失败，则需要重新再进行一次比较修改。
CAS（Compare-And-Swap，比较并交换）是原子指令的祖宗，也是整个无锁（lock-free）世界的地基。

ARMv8相关指令集：
```shell
LL/SC 实现（A64）
┌──────────────┐          ┌──────────────┐
│  LDAXR  x0,[x1]   ←─① 读独占   │  把 *addr 载入 x0，标记物理地址“独占”
│  CMP    x0,old    ←─② 比较     │  是否等于预期？
│  B.NE   fail                     │  不等直接失败
│  STLXR  w2,new,[x1] ←─③ 写独占 │  相等则尝试写入新值
│  CBNZ   w2,retry                 │  写独占失败（w2=1）重试
│  CLREX                           │  成功清除独占标记
└──────────────┘
```
### 1.1.4. 故障解决
#### 1.1.4.1. 自述
软件开发过程中最经典的一句话，解决不了问题就加一层。
作为开发，很容易在开发过程中一旦系统复杂之后，陷入技术细节的陷阱，而导致解决问题会复杂化且失去方向。一旦遇到问题阻塞了，应该整体梳理，大胆猜想，反向验证排除无关性。
把对应模块功能核心原理流程梳理清楚，实现复杂的边界在此，所以不能解决问题过程脱离该模块本身功能逻辑主线。
在解决mutex互斥锁时，用户态、内核、等待队列等不能开单独脱离开。
也就是经典的一句话：由复杂问题抽象至简单问题，这才是最难的。

#### 1.1.4.2. 互斥锁实现背景
POSIX语义实现futex，基于内核的互斥锁及等待队列接口，在适配层完成链路打通，因为保证内核有感知，实现符合POSIX语义的调度，进程的调度与之强相关。

#### 1.1.4.3. 故障解决

长稳测试：扩充的LTP测试用例 -> 触发故障 -> 修改测试用例缩小触发场景,提高复现概率 -> 明确两个进程高频竞争互斥锁就会触发 -> 添加日志打印，crash场景，日志来不及输出，因为这个本身是当前系统缺陷，没工具能够gdb调试适配层到内核层，所以只能正面看代码逻辑以及futex的POSIX语义，大胆猜测。后续发现内部实现涉及等待队列、实现具体系统调用接口时，且又涉及调度以及内核实现流程中还会有其他自旋锁以及原子变量的调用。
如何找到故障触发根因，这才是最大的难点？
本身与锁相关，然后根据已有日志打印进行分析。

临界区执行操作极少，导致锁的竞争异常激烈，对唤醒状态更新实时性要求极高，这个涉及内核态数据结构更新，用户态原子变量及状态字段更新。

测试用例两个线程在临界区执行操作：
            1、线程A 执行count++后释放。
            2、线程B 无任何操作直接释放。
        临界区的执行流程极少，导致锁的竞争更加频繁，对阻塞唤醒状态更新实时性要求极高。但在pi_futex的获取和释放时，内核FutexState的获取更新对应的等待队列存在并发场景，并不能保证更新FutexState在流程中作为临界资源完成活跃池的添加和查找的执行顺序，可能存在情况如下：
        线程B未完成阻塞操作，线程A提前唤醒，发现无待唤醒线程则释放锁更新value值为0直接退出，线程B此时才完成阻塞，此时线程A根据用户value值为0，不会执行唤醒线程B操作。
        同时反向论证：
        在测试用例的两个线程中的临界区添加空循环达到延长临界区执行效果，使测试线程获取pi_futex后，线程执行一定时长再释放锁（降低了锁的竞争频率，保证足够时间完成阻塞流程），修改后的测试用例反复验证测试无crash故障现象，且正常执行通过。
        说明故障触发场景可能就是上述猜想，则可以主要在内核对应的唤醒（PIWakeForStarnix）或者阻塞（PIWaitForStarnix）函数添加调试日志信息，查看错误场景pi_futex的value值、FutexState以及等待队列状态信息，（重点在pi_futex的value值改变位置与内核对象FutexState改变位置添加调试日志），根据日志定位：
从日志可以看出线程A调用PIWakeForStarnix判断futex_ref == nullptr正常退出了，但是线程B在调用PIWaitForStarnix这时候才真正加入等待队列完成阻塞操作。

根本原因就是在线程A获取FutexState失败，应立即更新value为0（释放锁）并返回用户，但是更新前被调度切换，线程B将A设置成锁的等待队列的owner，并完成阻塞操作，然后线程A才更新value值并返回。
        线程A退出时，需要去内核唤醒可能被其阻塞的线程，在内核查询到其存在一个pi_futex的等待队列（由线程B阻塞时设置），但是用户态value当前值为0（线程A释放锁时设置），内核返回EPERM错误码，从而导致线程A crash，同时线程B再也无法唤醒。体现到用户场景就是测试用例crash（线程A），并且无法结束（线程B）。

## 1.2. 协议栈优化

微内核涉及：内核、网卡驱动、网络协议栈、POSIX适配层等。
网络驱动通过内核IPC-FIFO进行数据转发至协议栈，协议栈通过内核IPC-socket传输给POSIX适配层，然后用户系统调用业务。这个过程中，协议栈本身还有数据包的封装解析、一层一层的向上转发。
缺点：调用链过长，需要通过iperf以及专用的打点工具分析链路中各个数据流转点的性能损耗。

### 1.2.1. 性能瓶颈点
协议栈通过IPC-FIFO从驱动拿到数据包，再分发给网络层处理之前，有个较长的处理流程：FIFO收包->数据包拷贝封装->抓包转发->二层解析->桥转发等。
### 1.2.2. 优化点
##### 1.2.2.0.1. 批处理
所以有必要对其进行批量化处理：批量从IPC-FIFO取包，批量封装以及桥转发。利用协议栈的数据处理能力，扩大多条流的同时处理，扩大缓冲区数据，处理数据包多条，达到批量处理。
##### 1.2.2.0.2. 同步IO进行异步改造
这里的IO主要是POSIX适配层与协议栈之间的控制域、数据域IO交互。在之前的架构中，控制域和数据域IO都是同步的，这样会导致反复的二次调用IPC来判断状态，多个IO控制层之间硬拷贝数据。由于IPC-SOCKET是非阻塞的，所以需要借用外部的waiter机制来进行阻塞读写，对IO进行异步改造就是引入一层异步IO，代替之前的同步IO封装层，由后台异步任务维护连接状态以及缓存数据。
##### 1.2.2.0.3. 协议栈升级
在go语言协议栈内部分段对带宽进行测试，发现在IP层开始，带宽性能开始出现大幅损耗。IP层主要作用包括寻址、路由选择、分片和重组、以及ICMP处理等，而fuchsia系统对golang语言的性能上存在瓶颈，考虑将协议栈升级到rust语言的netstack。

### 1.2.3. iperf3测试工具原理
测试原理：
* 采用客户端 - 服务器（C/S）架构，需先启动服务端监听端口，客户端主动发起连接。
* 测试时客户端按配置（如带宽上限、数据块大小）向服务端持续发送 TCP/UDP 数据包。
* 服务端接收数据后，计算传输速率、丢包率、延迟等指标，与客户端交互校验结果，最终输出统计报告。

网络带宽的核心指标：
. 关键核心指标
1. 吞吐量（Throughput）：实际每秒能传输的有效数据量（单位 Mbps/Gbps），是最核心的带宽指标，反映网络真实承载能力。
2. 带宽上限（Bandwidth）：网络理论支持的最大传输速率（如千兆网标称 1000Mbps），实际吞吐量会因损耗低于该值。
3. 丢包率（Packet Loss）：传输中丢失的数据包占总发送数的比例（UDP 测试重点），丢包率过高会导致数据重传、延迟增加。
4. 延迟（Latency）：数据包从客户端发送到服务端接收的总时间（单位 ms），含传输延迟、处理延迟等，影响实时性应用。
5. 抖动（Jitter）：延迟的波动范围（单位 ms），抖动大对语音、视频等实时业务影响显著。



## 1.3. POSIX适配层

让普通 Linux 二进制（ELF）不用修改、不用重编译，就能直接在 Fuchsia 上跑起来，并且“看起来”自己仍然跑在 Linux 内核里。

```shell
┌─────────────────────────────────────────┐
│  Linux 应用 (nginx, Redis, Android APK) │
│  ELF .so 依赖 glibc/musl                │
└────────────────┬────────────────────────┘
                 │ 系统调用 int 0x80 / svc #0
┌────────────────▼────────────────────────┐
│           Starnix 层                    │
│  ┌-------------------------------------┐ │
│  │ 1. syscall 分派 (linux_syscall.cpp)│ │  ← 把 Linux 编号转成内部 op
│  │ 2. 对象语义层                        │ │
│  │    · task → zx_process + zx_thread  │ │
│  │    · fd   → zx_handle (socket, vmo) │ │
│  │    · epoll→ zx_port                   │ │
│  │    · pipe → zx_socket(Stream)        │ │
│  │    · signalfd → zx_port + exception  │ │
│  │ 3. /proc, /sys, tmpfs, devpts        │ │  ← 虚拟文件系统
│  │ 4. 信号、ptrace、seccomp、futex       │ │
│  └-------------------------------------┘ │
└────────────────┬────────────────────────┘
                 │ zx 系统调用
┌────────────────▼────────────────────────┐
│           Zircon 微内核                 │
│  process, thread, vm_object, channel,   │
│  socket, port, interrupt, timer         │
└─────────────────────────────────────────┘

```

从 Linux 用户态 svc #0 → 内核 el1_sync → Starnix syscall 分派 → 返回 eret。

```shell
Linux 64-bit app (EL0)
┌──────────────────────────────┐
│ mov     x8, #83              │  // sys_mmap
│ svc     #0                   │  // 64-bit linux syscall
└────┬─────────────────────────┘
     V
CPU 同步异常向量 ───────────────┐
VBAR_EL1 + 0x400 (el0_sync)     │
┌─► kernel/arch/arm64/el0_sync.S│
│   kernel_exit 保存 pt_regs     │
│   stp x0, x1, [sp, #16 * 0]    │
│   mov x0, sp                   │  // pt_regs *
│   bl  starnix_el0_handler      │
└────┬───────────────────────────┘
     V
starnix/syscall/starnix_el0_handler()
┌────────────────────────────────────┐
│ regs->orig_x8 = x8                 │  // syscall nr
│ regs->x0 ... x5 = 参数              │
│ if (regs->x8 < 512)                │
│     ret = linux_syscall_table[x8](regs); │
│ else                               │
│     ret = -ENOSYS;                 │
│ regs->x0 = ret;                    │  // 返回值写回
│ kernel_exit 0, eret                │
└────┬───────────────────────────────┘
     V
eret // ELR_EL1 = regs->pc, SPSR_EL1 = 0x0 (EL0t)
回到用户态 EL0
```
restricted模式：
线程跑在EL1，但是看不见内核，只能看到该线程运行的资源。
```cpp
loader.restricted_elf_load()
├─ zx_restricted_bind(handle, entry, restricted_vmo_list[])
│  ├─ aspace_create_restricted()   // 新建“小页表”
│  │   仅映射：ELF、栈、vdso、restricted_vmo_list
│  ├─ thread_restricted_enter()
│  │   t->restricted_flag = 1
│  │   t->elr_el1 = entry
│  │   t->spsr_el1 = 0x205 (EL1t, SP0)
│  │   ttbr0_el1 = restricted_ttbr0  // 只有白名单物理页
│  │   ttbr1_el1 = 0                  // 不映射内核
└─ 首次调度 → eret 到 EL1 入口
```
结果：
PC 运行在 EL1，但页表里 没有 kernel text、没有 kstack、没有外设；
任何访问 0xffff... 内核地址 → stage-1 页表走空 → EL1 同步异常 → kill。

# 2. Linux


## 2.1. yocto构建

管理源代码、构建工程、添加组件、定制化裁剪。

## 2.2. RT补丁

RT 补丁（PREEMPT_RT）的核心作用：把 Linux 从“通用分时操作系统”变成“硬实时操作系统”。让关键任务在严格时间内完成响应，不会因为内核不可抢占、关中断、锁竞争而掉链子。

### 2.2.1. RT修改了什么

| 场景         | 原版 Linux                           | PREEMPT\_RT 后                            | 效果                              |
| ---------- | ---------------------------------- | ---------------------------------------- | ------------------------------- |
| **关中断临界区** | `local_irq_disable()` 屏蔽中断 100 ms+ | 把 **irq off** 变成 **irq thread**（线程化中断）   | 中断随时可响应，**延迟 < 10 µs**          |
| **自旋锁**    | `raw_spinlock` 忙等 + 关抢占            | 变成 **sleepable rt\_mutex**，可调度让出 CPU     | 高优先级任务可**强制抢占**持锁低优先级任务         |
| **定时器**    | 基于 **tick**（HZ）                    | 高精度 **hrtimer** + **NO\_HZ\_FULL**       | 周期抖动从毫秒级降到 **< 20 µs**          |
| **调度延迟**   | 内核态不可抢占（临界区、调度关闭）                  | 打开 **CONFIG\_PREEMPT\_RT** 后，**几乎全程可抢占** | 最坏调度延迟 **< 100 µs**（x86\_64 实测） |

二、RT 补丁 5 大关键技术点
中断线程化（threaded IRQ）
所有中断下半部变成 可调度线程，拥有 实时优先级 50-80，高优先级任务可随时抢占。
自旋锁 → rt_mutex
把 spinlock_t 内部换成 rt_mutex，支持优先级继承（PI），避免优先级反转。
关抢占区 → 可抢占
原版 preempt_disable() 区在 RT 下变成 “可抢占临界区”，通过 per-CPU 计数器 + rt_mutex 实现。
高精度时钟（hrtimer）
取代传统 jiffies，支持 纳秒级分辨率，配合 NO_HZ_FULL 消除周期时钟中断。
内存管理实时优化
把 mlockall() 和 vmalloc 的关中断区拆分，页面故障路径可抢占，避免 page fault 延迟抖动。

### 2.2.2. 各修改方案原理

#### 2.2.2.1. 硬中断线程化

#### 2.2.2.2. Mutex优先级继承
为了防止发生优先级反转，Mutex实现采用了rt_mutex的实现，具有优先级集成，同时对锁流程进行实时性优化改造。

优先级反转发生场景
1. 低优先级任务A获取锁资源
2. 中优先级任务B执行CPU密集型任务，抢占低优先级任务A
3. 高优先级任务C试图获取低优先级任务A持有的锁，但由于中优先级任务B抢占了低优先级任务A而阻塞
这种优先级反转会无限期延迟高优先级任务。



## 2.3. systemd及启动优化
systemd服务本身框架？


## 2.4. BSP集成及烧写


## 2.5. 安全启动-启动流程

# 3. Hypervisor

## 3.1. armv8虚拟化

## 3.2. 域间通信方案

## 3.3. 音视频实现方案


# 4. 个人优势
1. 学历背景及实验室：
2. 工作平台及项目经历：多个类型OS开发经验，微内核 -> linux -> hypervisor跨域融合复杂虚拟化架构
3. 行业标准和开发流程：
4. 操作系统基础知识：
5. arm硬件架构知识，硬件辅助虚拟化，知道各虚拟化方案的优缺点。
6. 技术前沿了解：
7. 工作中不仅注重研发功能本身，包括功能
8. 基于该背景，对于上层业务功能开发及底层开发都有整体系统框架的支撑
9. 故障定位及分析，会思考如何提高开发效率，下次针对同等问题进行通用方案解决
