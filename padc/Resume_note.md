# 微内核架构

## futex锁机制实现--用户态锁实现
### 难点
futex：整个实现调用链设计硬件原子\内核并发、调度器、内核锁实现机制（中断锁，关调度等）等。
只要一个环节与边界条件没对齐，就会出现丢唤醒、优先级反转等。这才是 futex 被称为“用户态一行代码，内核态十年补丁”的原因。

用户态对共享变量的写入，必须让内核在“检查该变量”与“把线程挂起”这两个动作之间看得见，否则就永远睡死。

### 丢唤醒
弱内存模型下，user 与 kernel 的内存序必须配对：
Linux 用 set_user() 里隐式 DMB，Fuchsia 要在 zx_futex_wait 入口显式 atomic_thread_fence(memory_order_seq_cst)，否则 *uaddr 检查与真正挂起之间可能重入，造成**“丢唤醒”**。

用户态对共享变量的写入，必须让内核再“检查该变量”与线程挂起这两个动作之间看得见，否则就会永远睡死。
五fence的坏序列（若内存模型CPU允许重排）
```cpp
// 用户态解锁路径
unlock() {
    lock->uaddr = 0;            // ① store 0
    // 没有内存屏障
}

// 用户态加锁路径
lock() {
    if (CAS(&lock->uaddr, 0, 1)) return;   // ② fast-path 成功
    // slow path
    for (;;) {
        int v = lock->uaddr;               // ③ load 当前值
        if (v != 0 &&
            syscall(SYS_futex, &lock->uaddr, FUTEX_WAIT, v, NULL) == 0)
            break;
    }
}
```
cpu允许把1的store延迟到之后执行。出现以下时许：
```shell
CPU-0 解锁线程                  CPU-1 加锁线程
─────────────────              ─────────────────
① store 0 进入 ST buffer
                               ③ load uaddr 拿到 1
                               进入内核：
                               ├─ 内核读 *uaddr 得到 1
                               ├─ 校验通过
                               ├─ 把线程设为 TASK_INTERRUPTIBLE
                               ├─ 调度走 —— 此刻**真正睡眠**
② ST buffer 终于刷到 cache
                               → 解锁线程的 0 才被别的 CPU 看见
                               → 没人再 wake，永远睡死
```
这就是典型的（Lost wake-up）内存检查值与挂起之间，用户态的写入被延迟或未完成，就被调度切换，导致检查通过但实际已生效，没人再唤醒，导致线程回收时，触发未被唤醒的线程死等待，系统崩溃。
所以保证内核与用户态的执行内存序对齐，让写入-刷回-检查-挂起按照顺序执行。

根本原因就是：弱内存模型 CPU 允许 store 延迟、load 提前，检查时值是对的，但真正的 0 还在 Store Buffer里，内核一挂起就永远没人再唤醒——睡死线程就是这么来的。

保证拿锁的线程唤醒时，在内核调用过程，整个过程不被调度，也就是睡眠路径根本不会被走到，丢唤醒自然消失。

### 扩展
CAS机制与CMP原子比较修改：读-改-写一次性原子决策，成功修改为期望新状态（值），失败，则需要重新再进行一次比较修改。
CAS（Compare-And-Swap，比较并交换）是原子指令的祖宗，也是整个无锁（lock-free）世界的地基。

ARMv8相关指令集：
```shell
LL/SC 实现（A64）
┌──────────────┐          ┌──────────────┐
│  LDAXR  x0,[x1]   ←─① 读独占   │  把 *addr 载入 x0，标记物理地址“独占”
│  CMP    x0,old    ←─② 比较     │  是否等于预期？
│  B.NE   fail                     │  不等直接失败
│  STLXR  w2,new,[x1] ←─③ 写独占 │  相等则尝试写入新值
│  CBNZ   w2,retry                 │  写独占失败（w2=1）重试
│  CLREX                           │  成功清除独占标记
└──────────────┘
```

## 协议栈优化

微内核涉及：内核、网卡驱动、网络协议栈、POSIX适配层等。
网络驱动通过内核IPC-FIFO进行数据转发至协议栈，协议栈通过内核IPC-socket传输给POSIX适配层，然后用户系统调用业务。这个过程中，协议栈本身还有数据包的封装解析、一层一层的向上转发。
缺点：调用链过长，需要通过iperf以及专用的打点工具分析链路中各个数据流转点的性能损耗。

### 性能瓶颈点
协议栈通过IPC-FIFO从驱动拿到数据包，再分发给网络层处理之前，有个较长的处理流程：FIFO收包->数据包拷贝封装->抓包转发->二层解析->桥转发等。
#### 优化点
##### 批处理
所以有必要对其进行批量化处理：批量从IPC-FIFO取包，批量封装以及桥转发。利用协议栈的数据处理能力，扩大多条流的同时处理，扩大缓冲区数据，处理数据包多条，达到批量处理。
##### 同步IO进行异步改造
这里的IO主要是POSIX适配层与协议栈之间的控制域、数据域IO交互。在之前的架构中，控制域和数据域IO都是同步的，这样会导致反复的二次调用IPC来判断状态，多个IO控制层之间硬拷贝数据。由于IPC-SOCKET是非阻塞的，所以需要借用外部的waiter机制来进行阻塞读写，对IO进行异步改造就是引入一层异步IO，代替之前的同步IO封装层，由后台异步任务维护连接状态以及缓存数据。
协议栈升级
在go语言协议栈内部分段对带宽进行测试，发现在IP层开始，带宽性能开始出现大幅损耗。IP层主要作用包括寻址、路由选择、分片和重组、以及ICMP处理等，而ZEOS系统对golang语言的性能上存在瓶颈，考虑将协议栈升级到rust语言的netstack。

## POSIX适配层





# Linux


## yocto构建

## RT补丁


## systemd及启动优化


## BSP集成及烧写


## 安全启动-启动流程

# Hypervisor

## armv8虚拟化

## 域间通信方案

## 音视频实现方案


# 个人优势
1. 学历背景及实验室：
2. 工作平台及项目经历：多个类型OS开发经验，微内核 -> linux -> hypervisor跨域融合复杂虚拟化架构
3. 行业标准和开发流程：
4. 操作系统基础知识：
5. arm硬件架构知识，硬件辅助虚拟化，知道各虚拟化方案的优缺点。
6. 技术前沿了解：
7. 工作中不仅注重研发功能本身，包括功能
8. 基于该背景，对于上层业务功能开发及底层开发都有整体系统框架的支撑
9. 故障定位及分析，会思考如何提高开发效率，下次针对同等问题进行通用方案解决

